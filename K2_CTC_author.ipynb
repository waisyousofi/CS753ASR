{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lMdkIqhoTJJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7114d9-76f5-4f31-eebe-07047a586ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting k2\n",
            "  Downloading k2-1.23.4-py3.9-none-any.whl (103.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from k2) (0.20.1)\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->k2) (4.5.0)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->k2) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->k2) (67.7.2)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, k2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed k2-1.23.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
            "Collecting environment information...\n",
            "PyTorch version: 1.13.1+cu117\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 11.7\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 20.04.5 LTS (x86_64)\n",
            "GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "Clang version: 10.0.0-4ubuntu1 \n",
            "CMake version: version 3.25.2\n",
            "Libc version: glibc-2.31\n",
            "\n",
            "Python version: 3.9.16 (main, Dec  7 2022, 01:11:51)  [GCC 9.4.0] (64-bit runtime)\n",
            "Python platform: Linux-5.10.147+-x86_64-with-glibc2.31\n",
            "Is CUDA available: False\n",
            "CUDA runtime version: 11.8.89\n",
            "CUDA_MODULE_LOADING set to: N/A\n",
            "GPU models and configuration: Could not collect\n",
            "Nvidia driver version: Could not collect\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.7.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.7.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.7.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.7.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.7.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.7.0\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.7.0\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] numpy==1.22.4\n",
            "[pip3] torch==1.13.1\n",
            "[pip3] torchaudio==2.0.1+cu118\n",
            "[pip3] torchdata==0.6.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtext==0.15.1\n",
            "[pip3] torchvision==0.15.1+cu118\n",
            "[conda] Could not collect\n"
          ]
        }
      ],
      "source": [
        "!pip install k2\n",
        "!python3 -m torch.utils.collect_env"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import _k2\n",
        "import k2\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "0wM40XelT7SM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_substraction_exp(a, b):\n",
        "    \"\"\" return (a.exp() - b.exp()).log(): a numeracal safe implementation \"\"\"\n",
        "    ans = torch.ones_like(a) * float(\"-inf\")\n",
        "\n",
        "    # avoid -inf in input\n",
        "    mask1 = torch.logical_and(~torch.isinf(a), ~torch.isinf(b))\n",
        "    a_ = torch.where(mask1, a, -1.0) # avoid any operation on -inf\n",
        "    b_ = torch.where(mask1, b, -2.0)\n",
        "\n",
        "    # avoid -inf in output: need to be very small as these values would be picked by mask1\n",
        "    ans_tmp = b_ + ((a_-b_).exp() - 1).log()\n",
        "    a_ = torch.where(torch.isinf(ans_tmp), -2000.0, a_)\n",
        "    b_ = torch.where(torch.isinf(ans_tmp), -2001.0, b_)\n",
        "\n",
        "    ans1 = b_ + ((a_-b_).exp() - 1).log()\n",
        "    ans = torch.where(mask1, ans1, ans)\n",
        " \n",
        "    mask2 = torch.logical_and(~torch.isinf(a), torch.isinf(b))\n",
        "    ans = torch.where(mask2, a, ans)\n",
        "\n",
        "    return ans"
      ],
      "metadata": {
        "id": "okyUrSH6yx7r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BEAM_SIZE=5\n",
        "class BPECTC(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 odim, \n",
        "                 eprojs, \n",
        "                 dropout_rate, \n",
        "                 reduce=True,\n",
        "                 log_semiring=True):\n",
        "        super().__init__()\n",
        "        # common CTC settings\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.loss = None\n",
        "        self.ctc_lo = torch.nn.Linear(eprojs, odim)\n",
        "        self.probs = None\n",
        "        self.reduce = reduce\n",
        "        self.log_semiring = log_semiring\n",
        "\n",
        "    def forward(self, hs_pad, hlens, ctc_graphs):\n",
        "        \"\"\"\n",
        "        hs_pad: [B, T, D], encoder output;\n",
        "        hlens: [B], effective length of each encoder output\n",
        "        ctc_graphs: list of matrix that specify ctc_graphs\n",
        "        \"\"\"\n",
        "        batch_size = hs_pad.size(0)\n",
        "     \n",
        "        supervision = torch.stack([torch.arange(batch_size),\n",
        "                                   torch.zeros(batch_size),\n",
        "                                   hlens.cpu()], dim=1).int()\n",
        "        indices = torch.argsort(supervision[:, 2], descending=True)\n",
        "        supervision = supervision[indices]\n",
        "\n",
        "        nnet_output = self.ctc_lo(F.dropout(hs_pad, p=self.dropout_rate))\n",
        "        nnet_log_prob = F.log_softmax(nnet_output, dim=-1)\n",
        "\n",
        "        dense_fsa_vec = k2.DenseFsaVec(nnet_log_prob, supervision)\n",
        "        #print(dense_fsa_vec)\n",
        "        #print(\",,,,,\")\n",
        "\n",
        "        ctc_graphs = self.revise_ctc_graphs_1(ctc_graphs, indices).requires_grad_(False)\n",
        " \n",
        "        lats = k2.intersect_dense(ctc_graphs, dense_fsa_vec, BEAM_SIZE)\n",
        "        forward_scores = - lats.get_tot_scores(use_double_scores=True, \n",
        "                                               log_semiring=self.log_semiring)\n",
        "        #print(forward_scores)\n",
        "\n",
        "        if self.reduce:\n",
        "            forward_scores = forward_scores.mean()\n",
        "        else:\n",
        "            forward_scores = forward_scores.sum()\n",
        "\n",
        "        # To be compatilble with previous ctc loss: make a recored\n",
        "        self.probs = F.softmax(nnet_output, dim=-1)\n",
        "        self.loss = forward_scores \n",
        "        return forward_scores\n",
        "\n",
        "    def revise_ctc_graphs(self, mats,indices):\n",
        "        \"\"\"\n",
        "        The ctc graphs should be revised to be compatible with other CTC implementation.\n",
        "        We assume:\n",
        "        (1) The phone 'sil' is never used so the sil_prob of 'lang' is always zero.\n",
        "        (2) After deleting the 'sil' in the ilabel table, the remained output units are \n",
        "            identical with the 'char_list' defined in output dictionary\n",
        "        \"\"\"\n",
        "        ctc_graphs = []\n",
        "        for mat in mats:\n",
        "            assert not torch.any(mat[:, 2] == 1), \"No sil should be used in CTC graph\"\n",
        "            mat[:, 2] = torch.where(mat[:, 2] <= 0, mat[:, 2], mat[:, 2] - 1) \n",
        "            ctc_graph = k2.Fsa.from_dict({\"arcs\": mat.detach()})\n",
        "            ctc_graphs.append(ctc_graph)\n",
        "        return k2.create_fsa_vec(ctc_graphs)\n",
        "    def revise_ctc_graphs_1(self,mats,indices):\n",
        "        ctc_graphs = []\n",
        "        for mat in mats:\n",
        "            ctc_graph = k2.Fsa.from_str(mat)\n",
        "            ctc_graphs.append(ctc_graph)\n",
        "            \n",
        "        return k2.create_fsa_vec(ctc_graphs)\n",
        "        \n",
        "    # Copy from ctc.py\n",
        "    def softmax(self, hs_pad):\n",
        "        self.probs = F.softmax(self.ctc_lo(hs_pad), dim=-1)\n",
        "        return self.probs\n",
        "\n",
        "    def log_softmax(self, hs_pad):\n",
        "        return F.log_softmax(self.ctc_lo(hs_pad), dim=-1)\n",
        "\n",
        "    def argmax(self, hs_pad):\n",
        "        return torch.argmax(self.ctc_lo(hs_pad), dim=-1)"
      ],
      "metadata": {
        "id": "zjpUeDj-UR1o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bayesctc\n",
        "class BayesianCTC(BPECTC):\n",
        "    def __init__(self,\n",
        "                 odim,\n",
        "                 eprojs,\n",
        "                 dropout_rate,\n",
        "                 reduce=True,\n",
        "                 log_semiring=True,\n",
        "                 risk_strategy=\"time\",\n",
        "                 risk_start=0.0,\n",
        "                 risk_factor=0.0,\n",
        "                 direction=\"right\",\n",
        "                 aggregate=\"time\",\n",
        "                 den_scale=0.0,\n",
        "                 path_constraint_delay=0,\n",
        "    ):\n",
        "        super().__init__(odim, eprojs, dropout_rate, reduce, log_semiring)\n",
        "                \n",
        "        self.strategy = risk_strategy\n",
        "        self.risk_factor = risk_factor\n",
        "        self.aggregate = aggregate\n",
        "        self.den_scale = den_scale \n",
        "        self.path_constraint = path_constraint_delay > 0\n",
        "        self.path_constraint_delay = path_constraint_delay\n",
        "\n",
        "        assert 0.0 <= den_scale <= 1.0\n",
        "      \n",
        "        assert risk_factor >= 0.0\n",
        "        assert (\"time\" in aggregate and \"time\" in risk_strategy) or \\\n",
        "               (\"char\" in aggregate and \"char\" in risk_strategy), \\\n",
        "               f\"aggregate is {aggregate} ; risk_strategy is {risk_strategy}\"\n",
        "\n",
        "        # If true, use the occupation probability as the grouping strategy\n",
        "        self.group_as_occupation = risk_strategy in [\"char_align\"]\n",
        "   \n",
        "    def forward(self, hs_pad, hlens, ys_pad, ali):\n",
        " \n",
        "        # As required by k2, reorder by hlens in descending order\n",
        "        indices = torch.argsort(hlens, descending=True)\n",
        "        hlens, hs_pad, ys_pad = hlens[indices], hs_pad[indices], ys_pad[indices]\n",
        "        if isinstance(ali, torch.Tensor) and ali.dim() == 2:\n",
        "            ali = ali[indices]\n",
        "        \n",
        "        # nnet_output\n",
        "        nnet_output = self.ctc_lo(F.dropout(hs_pad, p=self.dropout_rate))\n",
        "        # nnet_output.register_hook(grad_plot_hook) # to check the gradient\n",
        "        nnet_output = F.log_softmax(nnet_output, dim=-1)\n",
        "        \n",
        "        # compute the loss for each utterance\n",
        "        loss = self.forward2(nnet_output, hlens, ys_pad, ali)\n",
        "        # recover the original order\n",
        "        indices2 = torch.argsort(indices)\n",
        "        loss = loss[indices2]\n",
        "\n",
        "        return loss.mean()\n",
        " \n",
        "    def forward2(self, nnet_output, hlens, ys_pad, ali):\n",
        "        B, T, D = nnet_output.size()\n",
        "\n",
        "        # (1) build DenseFsaVec\n",
        "        supervision = torch.stack([torch.arange(B),\n",
        "                                   torch.zeros(B),\n",
        "                                   hlens.cpu()], dim=1).int()\n",
        "\n",
        "        dense_fsa_vec = k2.DenseFsaVec(nnet_output, supervision)\n",
        "\n",
        "        # (3) Intersection to get lattice and corresponding arc_maps\n",
        "        ys = [[x for x in y if x != -1] for y in ys_pad.cpu().tolist()]\n",
        "        ctc_graphs = k2.ctc_graph(ys).to(nnet_output.device)\n",
        "        olens = torch.Tensor([len(y) for y in ys]).to(nnet_output.device).long()\n",
        "        U = max(olens)\n",
        "\n",
        "        lats = k2.intersect_dense(ctc_graphs, dense_fsa_vec, BEAM_SIZE,\n",
        "                   seqframe_idx_name=\"seqframe_idx\",\n",
        "                   frame_idx_name=\"frame_idx\",\n",
        "        )\n",
        "\n",
        "        # Since arc_maps are not accessible for K2 user API, we do the second intersection\n",
        "        # with _k2 object to obtain them\n",
        "        ragged_lat, arc_map_a, arc_map_b = _k2.intersect_dense(\n",
        "          a_fsas=ctc_graphs.arcs,\n",
        "          b_fsas=dense_fsa_vec.dense_fsa_vec,\n",
        "          a_to_b_map=None,\n",
        "          output_beam=BEAM_SIZE\n",
        "        )\n",
        "\n",
        "        if self.path_constraint:\n",
        "            ali = ali // 4 + self.path_constraint_delay # due to subsampling; should avoid this hard code\n",
        "            with torch.no_grad():\n",
        "                arc_mask = self.get_constraint_ctc_mask(\n",
        "                    ctc_graphs, dense_fsa_vec, arc_map_a, arc_map_b, ali,\n",
        "                )\n",
        "            lats.scores = torch.where(arc_mask, -10000.0, lats.scores.double()).float()\n",
        "            return - lats.get_tot_scores(True, True)\n",
        "\n",
        "        # (5) Find all states whose incoming label is non-blank \n",
        "        with torch.no_grad():\n",
        "            # Find all index from two side to ensure safety.\n",
        "            # You may disable either side to make it faster, after we are sure they are identical\n",
        "            # state_idx_f, u_idx_f, t_idx_f, fsa_idx_f = self.find_forward_index2( \n",
        "            #       ragged_lat, ctc_graphs, dense_fsa_vec, arc_map_a, arc_map_b, lats.frame_idx\n",
        "            # )\n",
        "            \n",
        "            state_idx, u_idx, t_idx, fsa_idx = self.find_backward_index2(\n",
        "                  ragged_lat, ctc_graphs, dense_fsa_vec, arc_map_a, arc_map_b, lats.frame_idx\n",
        "            )\n",
        "            # assert torch.all(state_idx_f == state_idx)\n",
        "            # assert torch.all(u_idx_f == u_idx)\n",
        "            # assert torch.all(t_idx_f == t_idx)\n",
        "            # assert torch.all(fsa_idx_f == fsa_idx)\n",
        "\n",
        "        # (6) Probability of each path group\n",
        "        forward_scores  = lats.get_forward_scores(True, True)\n",
        "        backward_scores = lats.get_backward_scores(True, True)\n",
        "\n",
        "        alpha_ = forward_scores[state_idx]\n",
        "        alpha = torch.ones([B, U, T]).double().to(nnet_output.device) * float('-inf')\n",
        "        alpha[fsa_idx, u_idx, t_idx] = alpha_\n",
        "\n",
        "        beta_ = backward_scores[state_idx]\n",
        "        beta = torch.ones([B, U, T]).double().to(nnet_output.device) * float('-inf')\n",
        "        beta[fsa_idx, u_idx, t_idx] = beta_\n",
        "\n",
        "        if self.group_as_occupation:\n",
        "            loss_state = alpha + beta\n",
        "        else:\n",
        "            p_idx = ys_pad[fsa_idx, u_idx].long()\n",
        "            p_ = nnet_output[fsa_idx, t_idx, p_idx].double()\n",
        "            p = torch.ones([B, U, T]).double().to(nnet_output.device) * float('-inf')\n",
        "            p[fsa_idx, u_idx, t_idx] = p_\n",
        "       \n",
        "            beta_prime = log_substraction_exp(beta[:, :, :-1], beta[:, :, 1:] + p[:, :, 1:])\n",
        "            beta_prime = torch.cat([beta_prime, beta[:, :, -1:]], dim=-1)\n",
        "\n",
        "            loss_state = alpha + beta_prime\n",
        "\n",
        "        # (7) Bayeisan Risk function.\n",
        "        loss_state = loss_state + self.get_risk_scores(loss_state, hlens, olens, ali)\n",
        "        loss_state = torch.where(torch.isnan(loss_state), float('-inf'), loss_state) # should never see nan\n",
        "\n",
        "        # (8) Aggregate scores along time or char axis\n",
        "        if self.aggregate in [\"time\", \"time_whole\"]:\n",
        "            loss_u = torch.logsumexp(loss_state, dim=2)\n",
        "            if self.den_scale > 0.0:\n",
        "                loss_u = loss_u - self.den_scale * lats.get_tot_scores(True, True).unsqueeze(1)\n",
        "            mask = torch.isinf(loss_u)\n",
        "            if self.aggregate in [\"time_whole\"]:\n",
        "                loss_fsas = torch.where(mask, 0.0, loss_u).sum(1) / (~mask).double().sum(1)\n",
        "            elif self.aggregate == \"time\":\n",
        "                loss_fsas = loss_u[torch.arange(B).long(), (~mask).long().sum(1)-1]\n",
        "\n",
        "        elif self.aggregate in [\"char\", \"char_whole\"]:\n",
        "            loss_t = torch.logsumexp(loss_state, dim=1)\n",
        "            mask = torch.isinf(loss_t)\n",
        "            if self.aggregate == \"char_whole\":\n",
        "                loss_fsas = torch.where(mask, 0.0, loss_t).sum(1) / (~mask).double().sum(1)\n",
        "            elif self.aggregate == \"char\":\n",
        "                loss_fsas = loss_t[torch.arange(B).long(), (~mask).long().sum(1)-1]\n",
        "        \n",
        "        else:\n",
        "            raise NotImplementedError \n",
        "\n",
        "        # Fix the invalid loss: input length < output length\n",
        "        # This will happen when the label is (wrongly) too long and the inter_ctc has trimmed too harsh\n",
        "        loss_fsas = torch.where(hlens < olens, 0.0, loss_fsas)\n",
        "        if torch.any(hlens < olens):\n",
        "            print(f\"Invalid data: input shorter than output with data index: {(hlens < olens).nonzero()}\")\n",
        "\n",
        "        # (10) Info for debug   \n",
        "        if torch.any(torch.isinf(loss_fsas)):\n",
        "            err_idx = torch.nonzero(torch.isinf(loss_fsas).int()).squeeze(0)\n",
        "            print(\"An bad case detected. Override so the training can continue. debug info: \")\n",
        " \n",
        "        return - loss_fsas # for minimization\n",
        "\n",
        "    def get_risk_scores(self, loss_state, hlens, olens, ali):\n",
        "        \"\"\" Add the bayesian risk in multiple ways \"\"\"\n",
        "        B, U, T = loss_state.size()\n",
        "\n",
        "        if self.strategy == \"time\": \n",
        "            risk = torch.arange(1, T+1, device=loss_state.device).unsqueeze(0).unsqueeze(0).repeat(B, U, 1)\n",
        "            risk = risk / hlens.unsqueeze(1).unsqueeze(1) * self.risk_factor\n",
        "             \n",
        "        elif self.strategy == \"time_relative\":\n",
        "            risk = torch.arange(1, T+1, device=loss_state.device).unsqueeze(0).unsqueeze(0).repeat(B, U, 1)\n",
        "            max_stamp = torch.argmax(loss_state, dim=2, keepdim=True)\n",
        "            risk = (risk - max_stamp) / hlens.unsqueeze(1).unsqueeze(1) * self.risk_factor\n",
        "\n",
        "        elif self.strategy == \"char_align\":\n",
        "            ali = ali // 4 # sub-sampling\n",
        "            torch.set_printoptions(threshold=1e8)\n",
        "            risk = torch.arange(1, T+1, device=loss_state.device).unsqueeze(0).unsqueeze(0).repeat(B, U, 1)\n",
        "            risk = torch.abs(risk - ali.unsqueeze(2)) / hlens.unsqueeze(1).unsqueeze(1) * self.risk_factor\n",
        "            print(\"risk is : \", risk, self.risk_factor)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return - risk        \n",
        "\n",
        "    def find_forward_index2(self, ragged_lat, a_fsas, b_fsas, arc_map_a, arc_map_b, frame_idx):\n",
        "        \"\"\" Find all states whose scores are exact alpha(t,u) \n",
        "            Return: state_idx, u_idx, t_idx, fsa_idx\n",
        "        \"\"\"\n",
        "\n",
        "        # (1) Using incoming format to compute alphas\n",
        "        ragint_lat = k2.Fsa(ragged_lat)._get_incoming_arcs()\n",
        "        ragint_graph = a_fsas._get_incoming_arcs()\n",
        "\n",
        "        # (2) For CTC graphs: Find all arc_ids for the incoming arcs of odd-id states\n",
        "        graph_arc_id_start = ragint_graph.shape.row_splits(2)[1::2]\n",
        "        graph_arc_id_end   = ragint_graph.shape.row_splits(2)[2::2]\n",
        "        graph_arc_ids = [torch.arange(graph_arc_id_start[i], graph_arc_id_end[i])\n",
        "                             for i in range(len(graph_arc_id_end)) \\\n",
        "                             if 2*(i+1) not in ragint_graph.shape.row_splits(1)[1:].cpu().tolist()] \n",
        "        graph_arc_ids = torch.cat(graph_arc_ids).to(a_fsas.device)\n",
        "        graph_arc_ids = ragint_graph.values.long()[graph_arc_ids]\n",
        "\n",
        "        # (3) For lattice: find the corresponding arcs in lattice\n",
        "        lat_arc_ids = (graph_arc_ids.unsqueeze(0) == arc_map_a.unsqueeze(1)).int().sum(dim=-1) # very memory consuming\n",
        "        # start, parts, interval = 0, [], 3e8 // len(arc_map_a)\n",
        "        # while start < len(graph_arc_ids):\n",
        "        #     part_in = graph_arc_ids[start: min(start+interval, len(graph_arc_ids))]\n",
        "        #     part_out = (part_in.unsqueeze(0) == arc_map_a.unsqueeze(1)).int().sum(dim=-1)\n",
        "        #     parts.append(part_out)\n",
        "        #     start += interval\n",
        "        # lat_arc_ids = torch.cat(parts, dim=0)\n",
        "        lat_arc_ids = lat_arc_ids.bool().nonzero(as_tuple=True)[0]\n",
        "\n",
        "        # (4) For lattice: find the corresponding states in lattice\n",
        "        state_ids = ragint_lat.shape.row_ids(2)[ragint_lat.values.long()]\n",
        "        state_ids = torch.unique(state_ids[lat_arc_ids]).long()\n",
        "\n",
        "        # (5) Find the fsa-ids\n",
        "        fsa_idx = ragged_lat.shape().row_ids(1)[state_ids].long()\n",
        "\n",
        "        # (6) Find the t-idx\n",
        "        t_idx = arc_map_b[(ragint_lat.shape.row_splits(2)[state_ids]).long()] // b_fsas.dense_fsa_vec.scores_dim1()\n",
        "        t_idx = (t_idx - b_fsas.dense_fsa_vec.shape().row_splits(1)[:-1][fsa_idx]).long()\n",
        "\n",
        "        # (7) Find the u-idx\n",
        "        lat_arc_ids = ragint_lat.shape.row_splits(2)[state_ids].long()\n",
        "        lat_arc_ids = ragint_lat.values.long()[lat_arc_ids]\n",
        "        graph_arc_ids = arc_map_a[lat_arc_ids].long()\n",
        "        u_idx = (a_fsas.arcs.values()[graph_arc_ids][:, 1] - 1).long() // 2\n",
        "\n",
        "        return state_ids, u_idx, t_idx, fsa_idx\n",
        "\n",
        "    def find_backward_index2(self, ragged_lat, a_fsas, b_fsas, arc_map_a, arc_map_b, frame_idx):\n",
        "        \"\"\" Find all states whose scores are exact beta(t,u)\n",
        "            Return state_idx, u_idx, t_idx, fsa_idx\n",
        "\n",
        "        \"\"\"     \n",
        "        # (1) Get the shape of graphs and lattice\n",
        "        ragint_lat = k2.Fsa(ragged_lat).arcs.shape()\n",
        "        ragint_graph = a_fsas.arcs.shape()\n",
        "\n",
        "        # (2) For CTC graphs: Find all arc_ids for the incoming arcs of odd-id states\n",
        "        graph_arc_id_start = ragint_graph.row_splits(2)[1::2]\n",
        "        graph_arc_id_end   = ragint_graph.row_splits(2)[2::2]\n",
        "        graph_arc_ids = [torch.arange(graph_arc_id_start[i], graph_arc_id_end[i])\n",
        "                             for i in range(len(graph_arc_id_end))]\n",
        "        graph_arc_ids = torch.cat(graph_arc_ids).to(a_fsas.device)\n",
        "\n",
        "        # (3) For lattice: find the corresponding arcs in lattice: the 2-dim tensor can be very large\n",
        "        # lat_arc_ids = (graph_arc_ids.unsqueeze(0) == arc_map_a.unsqueeze(1)).int().sum(dim=-1)\n",
        "        start, parts, interval = 0, [], int(2e7 / len(graph_arc_ids))\n",
        "        while start < len(arc_map_a):\n",
        "            part_in = arc_map_a[start: min(start+interval, len(arc_map_a))]\n",
        "            part_out = (graph_arc_ids.unsqueeze(0) == part_in.unsqueeze(1)).int().sum(dim=-1)\n",
        "            parts.append(part_out)\n",
        "            start += interval\n",
        "        lat_arc_ids = torch.cat(parts, dim=0)\n",
        "        lat_arc_ids = lat_arc_ids.bool().nonzero(as_tuple=True)[0]\n",
        "\n",
        "        # (4) For lattice: find the corresponding states in lattice\n",
        "        state_ids = torch.unique(ragint_lat.row_ids(2)[lat_arc_ids]).long()\n",
        "\n",
        "        # (5) Find the fsa-ids\n",
        "        fsa_idx = ragged_lat.shape().row_ids(1)[state_ids].long()\n",
        "\n",
        "        # (6) Find the t-idx\n",
        "        t_idx = arc_map_b[(ragint_lat.row_splits(2)[state_ids]).long()] // b_fsas.dense_fsa_vec.scores_dim1()\n",
        "        t_idx = (t_idx - b_fsas.dense_fsa_vec.shape().row_splits(1)[:-1][fsa_idx]).long() - 1\n",
        "\n",
        "        # (7) Find the u-idx\n",
        "        lat_arc_ids = ragint_lat.row_splits(2)[state_ids].long()\n",
        "        graph_arc_ids = arc_map_a[lat_arc_ids].long()\n",
        "        u_idx = (a_fsas.arcs.values()[graph_arc_ids][:, 0] - 1).long() // 2\n",
        "\n",
        "        return state_ids, u_idx, t_idx, fsa_idx\n",
        "\n",
        "    def get_constraint_ctc_mask(self, ctc_graph, dense_fsa_vec, arc_map_a, arc_map_b, ali):\n",
        "        blank_state_u_id = -1 \n",
        "        safe_t = 10000\n",
        "        num_fsas = len(ctc_graph.arcs.row_splits(1)) - 1\n",
        "\n",
        "        ## Find the index of u, b, t for each lattice arc\n",
        "        # find the dst state-id for each ctc_graph arc\n",
        "        arc_to_dst = ctc_graph.as_dict()[\"arcs\"][2 * num_fsas + 4:].view(-1, 4)[:, 1].long()\n",
        "       \n",
        "        # even states are the blank states; odd-states are the non-blank states\n",
        "        u_idx = torch.where(arc_to_dst % 2 == 0, blank_state_u_id, (arc_to_dst - 1) // 2)\n",
        "\n",
        "        # the ending states should also not be considered\n",
        "        ctc_ragged = ctc_graph._get_incoming_arcs()\n",
        "        inverse_shape = ctc_ragged.shape\n",
        "        incoming_arc_start = inverse_shape.row_splits(2)[inverse_shape.row_splits(1)[1:].long() - 1 ]\n",
        "        incoming_arc_end   = inverse_shape.row_splits(2)[inverse_shape.row_splits(1)[1:].long()]\n",
        "        incoming_arcs = [torch.arange(incoming_arc_start[i], incoming_arc_end[i]) for i in range(num_fsas)]\n",
        "        incoming_arcs = torch.cat(incoming_arcs).long()\n",
        "        incoming_arcs = ctc_ragged.values[incoming_arcs].long()\n",
        "        u_idx[incoming_arcs] = blank_state_u_id\n",
        "\n",
        "        # u_id: non-blank token id of each lattice arc\n",
        "        u_idx = u_idx[arc_map_a.long()]\n",
        "\n",
        "        # b_idx: batch id of each lattice arc\n",
        "        fsa_boundaries = ctc_graph.arcs.shape().row_splits(2).long()[ctc_graph.arcs.shape().row_splits(1).long()]\n",
        "        arc_ids = torch.arange(ctc_graph.num_arcs, device=ctc_graph.device)\n",
        "        b_idx = torch.bucketize(arc_ids, fsa_boundaries, right=True) - 1\n",
        "        b_idx = b_idx[arc_map_a.long()]\n",
        "\n",
        "        # t_idx: frame id of each lattice arc\n",
        "        b_shape = dense_fsa_vec.dense_fsa_vec.shape()\n",
        "        feat_dim, duration = dense_fsa_vec.scores.size(1), dense_fsa_vec.duration + 1\n",
        "        t_idx = arc_map_b // feat_dim\n",
        "        t_shift = torch.zeros(1 + len(duration), device=ctc_graph.device, dtype=duration.dtype)\n",
        "        t_shift[1:] = torch.cumsum(duration, dim=0)\n",
        "        t_idx = t_idx - t_shift[b_idx]\n",
        "\n",
        "        ## see if the given arcs should be killed\n",
        "        # the non-blank arcs whose t_idx is larger than the time threshold should be kill\n",
        "        t_threshold = ali[b_idx, u_idx]\n",
        "        t_threshold = torch.where(u_idx == -1, safe_t, t_threshold)\n",
        "        mask = t_idx > t_threshold\n",
        "\n",
        "        return mask\n"
      ],
      "metadata": {
        "id": "l34ejh00ysSV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "odim = 10  # output dimension\n",
        "eprojs = 16  # encoder projection dimension\n",
        "dropout_rate = 0.5  # dropout rate\n",
        "reduce = True  # whether to reduce the loss over the batch\n",
        "log_semiring = True  # whether to use log semiring\n",
        "ctc_loss = BPECTC(odim, eprojs, dropout_rate, reduce, log_semiring)"
      ],
      "metadata": {
        "id": "2tfMTTJRRuLh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hs_pad = torch.randn(4, 10, 16)  # batch size 32, input length 100, feature dimension 256\n",
        "hlens = torch.randint(1, 10, (4,))  # effective length of each encoder output\n",
        "\n",
        "# ctc_graphs = [\n",
        "#     torch.tensor([[0, 1, 10, 1036831949],[1, 2, 20, 1036831949],[2, 3, 30,1036831949],[2, 4, 40,1036831949],[3, 5, 50,1036831949],[4, 5, 60,1036831949],[5, 6, 70,1036831949],[6,7,-1,1036831949]], dtype=torch.int32)\n",
        "# ]\n",
        "s = '''\n",
        "0 3 1 0.1\n",
        "0 4 2 0.2\n",
        "0 5 3 0.7\n",
        "1 3 4 0.1\n",
        "1 4 5 0.2\n",
        "1 5 6 0.7\n",
        "2 3 7 0.1\n",
        "2 4 8 0.2\n",
        "2 5 9 0.7\n",
        "3 6 10 0.1\n",
        "3 7 11 0.2\n",
        "3 8 12 0.7\n",
        "4 6 13 0.1\n",
        "4 7 14 0.2\n",
        "4 8 15 0.7\n",
        "5 6 16 0.1\n",
        "5 7 17 0.2\n",
        "5 8 18 0.7\n",
        "6 9 -1 1\n",
        "7 9 -1 1\n",
        "8 9 -1 1\n",
        "9\n",
        "'''\n",
        "ctc_graph = k2.Fsa.from_str(s)\n",
        "\n",
        "\n",
        "ctc_graphs = [s]\n",
        "loss = ctc_loss(hs_pad, hlens, ctc_graphs)\n",
        "\n",
        "ctc_graph.draw('simple_fsa.svg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "cL6BFiP1SKtA",
        "outputId": "2221cab9-af43-440a-a254-f71f840058ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: WFSA Pages: 1 -->\n<svg width=\"335pt\" height=\"351pt\"\n viewBox=\"0.00 0.00 335.00 351.12\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 347.12)\">\n<title>WFSA</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-347.12 331,-347.12 331,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"18\" cy=\"-309.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-305.42\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n</g>\n<!-- 3 -->\n<g id=\"node2\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"112\" cy=\"-146.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"112\" y=\"-142.42\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n</g>\n<!-- 0&#45;&gt;3 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M36,-306.1C49.61,-302.78 68.16,-296.11 79,-283.12 85.98,-274.76 98.77,-211.53 105.89,-173.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"109.41,-174.13 107.81,-163.66 102.53,-172.84 109.41,-174.13\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-304.92\" font-family=\"Times,serif\" font-size=\"14.00\">1/0.1</text>\n</g>\n<!-- 4 -->\n<g id=\"node3\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"112\" cy=\"-81.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"112\" y=\"-77.42\" font-family=\"Times,serif\" font-size=\"14.00\">4</text>\n</g>\n<!-- 0&#45;&gt;4 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M20.07,-291.23C22.32,-266.77 29.32,-222.44 51,-192.12 59.75,-179.89 70.23,-185.34 79,-173.12 93.53,-152.89 85.56,-142.56 94,-119.12 95.49,-114.99 97.23,-110.68 99.01,-106.51\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"102.3,-107.73 103.18,-97.18 95.91,-104.88 102.3,-107.73\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-195.92\" font-family=\"Times,serif\" font-size=\"14.00\">2/0.2</text>\n</g>\n<!-- 5 -->\n<g id=\"node4\" class=\"node\">\n<title>5</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"112\" cy=\"-271.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"112\" y=\"-267.42\" font-family=\"Times,serif\" font-size=\"14.00\">5</text>\n</g>\n<!-- 0&#45;&gt;5 -->\n<g id=\"edge3\" class=\"edge\">\n<title>0&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M33.61,-318.91C46.28,-326.07 64.76,-333.23 79,-325.12 89.58,-319.09 97.03,-308.03 102.05,-297.48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"105.35,-298.68 106.01,-288.1 98.9,-295.95 105.35,-298.68\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-331.92\" font-family=\"Times,serif\" font-size=\"14.00\">3/0.7</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"213\" cy=\"-214.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"213\" y=\"-210.42\" font-family=\"Times,serif\" font-size=\"14.00\">6</text>\n</g>\n<!-- 3&#45;&gt;6 -->\n<g id=\"edge10\" class=\"edge\">\n<title>3&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116.47,-163.81C120.5,-179.15 128.79,-200.52 145,-211.12 156.57,-218.69 171.83,-219.88 184.85,-218.98\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"185.6,-222.42 195.15,-217.83 184.82,-215.47 185.6,-222.42\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-222.92\" font-family=\"Times,serif\" font-size=\"14.00\">10/0.1</text>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"213\" cy=\"-54.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"213\" y=\"-50.42\" font-family=\"Times,serif\" font-size=\"14.00\">7</text>\n</g>\n<!-- 3&#45;&gt;7 -->\n<g id=\"edge11\" class=\"edge\">\n<title>3&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.56,-129.97C123.61,-123.26 127.09,-115.37 130,-108.12 137.39,-89.73 129.87,-78.93 145,-66.12 155.89,-56.9 171.41,-53.71 184.77,-52.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"184.96,-56.41 194.89,-52.71 184.82,-49.41 184.96,-56.41\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-69.92\" font-family=\"Times,serif\" font-size=\"14.00\">11/0.2</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"213\" cy=\"-119.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"213\" y=\"-115.42\" font-family=\"Times,serif\" font-size=\"14.00\">8</text>\n</g>\n<!-- 3&#45;&gt;8 -->\n<g id=\"edge12\" class=\"edge\">\n<title>3&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M124.83,-133.3C130.4,-128.18 137.47,-122.86 145,-120.12 157.52,-115.57 172.23,-114.89 184.68,-115.53\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"184.68,-119.04 194.93,-116.37 185.25,-112.06 184.68,-119.04\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-123.92\" font-family=\"Times,serif\" font-size=\"14.00\">12/0.7</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge13\" class=\"edge\">\n<title>4&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.82,-97.18C123.92,-103.87 127.35,-111.78 130,-119.12 138.44,-142.56 129.49,-153.63 145,-173.12 156.02,-186.97 164.91,-182.87 180,-192.12 183.08,-194.01 186.28,-196.07 189.4,-198.16\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"187.64,-201.19 197.88,-203.94 191.59,-195.41 187.64,-201.19\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-195.92\" font-family=\"Times,serif\" font-size=\"14.00\">13/0.1</text>\n</g>\n<!-- 4&#45;&gt;7 -->\n<g id=\"edge14\" class=\"edge\">\n<title>4&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M115.03,-63.32C118.13,-45.44 125.86,-18.76 145,-6.12 157.98,2.45 166.29,1.22 180,-6.12 189.18,-11.03 196.17,-19.86 201.25,-28.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"198.17,-30.28 205.86,-37.56 204.39,-27.07 198.17,-30.28\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-9.92\" font-family=\"Times,serif\" font-size=\"14.00\">14/0.2</text>\n</g>\n<!-- 4&#45;&gt;8 -->\n<g id=\"edge15\" class=\"edge\">\n<title>4&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M130.12,-83.02C144.03,-85.02 163.9,-88.9 180,-96.12 183.52,-97.7 187.03,-99.7 190.38,-101.85\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"188.54,-104.84 198.73,-107.74 192.58,-99.12 188.54,-104.84\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-99.92\" font-family=\"Times,serif\" font-size=\"14.00\">15/0.7</text>\n</g>\n<!-- 5&#45;&gt;6 -->\n<g id=\"edge16\" class=\"edge\">\n<title>5&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M127.44,-280.6C141.82,-288.72 163.98,-297.59 180,-287.12 195.2,-277.19 203.23,-258.25 207.44,-242.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"210.92,-242.63 209.69,-232.11 204.09,-241.1 210.92,-242.63\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-294.92\" font-family=\"Times,serif\" font-size=\"14.00\">16/0.1</text>\n</g>\n<!-- 5&#45;&gt;7 -->\n<g id=\"edge17\" class=\"edge\">\n<title>5&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M113.65,-252.92C115.41,-228.08 121.85,-183.33 145,-154.12 155.99,-140.25 168.54,-148.61 180,-135.12 193.11,-119.7 187.53,-110.93 195,-92.12 196.62,-88.04 198.43,-83.76 200.25,-79.6\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"203.53,-80.82 204.41,-70.27 197.14,-77.97 203.53,-80.82\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-157.92\" font-family=\"Times,serif\" font-size=\"14.00\">17/0.2</text>\n</g>\n<!-- 5&#45;&gt;8 -->\n<g id=\"edge18\" class=\"edge\">\n<title>5&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M129.8,-267.1C145.09,-262.58 167.16,-253.75 180,-238.12 182.97,-234.51 196.82,-180.28 205.3,-146.29\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"208.72,-147.02 207.74,-136.47 201.93,-145.33 208.72,-147.02\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-264.92\" font-family=\"Times,serif\" font-size=\"14.00\">18/0.7</text>\n</g>\n<!-- 1 -->\n<g id=\"node5\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"18\" cy=\"-165.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-161.42\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n</g>\n<!-- 1&#45;&gt;3 -->\n<g id=\"edge4\" class=\"edge\">\n<title>1&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M35.93,-161.36C40.79,-160.29 46.1,-159.15 51,-158.12 61.8,-155.86 73.72,-153.46 84.07,-151.4\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"85.04,-154.78 94.17,-149.4 83.68,-147.91 85.04,-154.78\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-161.92\" font-family=\"Times,serif\" font-size=\"14.00\">4/0.1</text>\n</g>\n<!-- 1&#45;&gt;4 -->\n<g id=\"edge5\" class=\"edge\">\n<title>1&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M28.78,-150.51C31.26,-146.56 33.84,-142.24 36,-138.12 43.87,-123.14 38.72,-114.76 51,-103.12 60.06,-94.53 72.75,-89.24 84.13,-86.01\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"85.18,-89.36 94.07,-83.59 83.52,-82.56 85.18,-89.36\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-106.92\" font-family=\"Times,serif\" font-size=\"14.00\">5/0.2</text>\n</g>\n<!-- 1&#45;&gt;5 -->\n<g id=\"edge6\" class=\"edge\">\n<title>1&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M21.37,-182.9C24.78,-201.72 32.79,-231.14 51,-249.12 59.88,-257.9 72.55,-263.21 83.96,-266.4\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"83.4,-269.87 93.94,-268.79 85.03,-263.06 83.4,-269.87\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-267.92\" font-family=\"Times,serif\" font-size=\"14.00\">6/0.7</text>\n</g>\n<!-- 2 -->\n<g id=\"node6\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"18\" cy=\"-111.12\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-107.42\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge7\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M35.1,-116.86C47.21,-121.21 64.2,-127.4 79,-133.12 81.14,-133.95 83.35,-134.82 85.56,-135.7\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"84.51,-139.04 95.09,-139.53 87.12,-132.55 84.51,-139.04\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-136.92\" font-family=\"Times,serif\" font-size=\"14.00\">7/0.1</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge8\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M22.05,-93.35C25.83,-77.38 33.99,-54.85 51,-44.12 61.53,-37.48 67.68,-38.95 79,-44.12 85.54,-47.11 91.25,-52.17 95.97,-57.6\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"93.28,-59.84 102.15,-65.63 98.83,-55.57 93.28,-59.84\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-47.92\" font-family=\"Times,serif\" font-size=\"14.00\">8/0.2</text>\n</g>\n<!-- 2&#45;&gt;5 -->\n<g id=\"edge9\" class=\"edge\">\n<title>2&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M29.36,-125.46C31.84,-129.41 34.26,-133.8 36,-138.12 48.36,-168.85 32.61,-183.57 51,-211.12 59.35,-223.63 68.02,-219.85 79,-230.12 84.8,-235.55 90.4,-242.09 95.24,-248.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"92.56,-250.6 101.33,-256.54 98.18,-246.43 92.56,-250.6\"/>\n<text text-anchor=\"middle\" x=\"65\" y=\"-233.92\" font-family=\"Times,serif\" font-size=\"14.00\">9/0.7</text>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"305\" cy=\"-119.12\" rx=\"18\" ry=\"18\"/>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"305\" cy=\"-119.12\" rx=\"22\" ry=\"22\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-115.42\" font-family=\"Times,serif\" font-size=\"14.00\">9</text>\n</g>\n<!-- 6&#45;&gt;9 -->\n<g id=\"edge19\" class=\"edge\">\n<title>6&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M226.14,-201.31C240.41,-186.24 264.39,-160.94 281.97,-142.38\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"284.73,-144.55 289.07,-134.89 279.65,-139.74 284.73,-144.55\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-180.92\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1/1</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge20\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M228.06,-64.28C241.41,-73.92 261.8,-88.64 278.09,-100.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"276.49,-103.57 286.65,-106.59 280.59,-97.9 276.49,-103.57\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-95.92\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1/1</text>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge21\" class=\"edge\">\n<title>8&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M231.35,-119.12C243.08,-119.12 258.89,-119.12 272.76,-119.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"272.9,-122.62 282.9,-119.12 272.9,-115.62 272.9,-122.62\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-122.92\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1/1</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fc592619160>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m-aZKoYkA7y",
        "outputId": "d8404323-f456-4fcc-b55d-ce0ae7b56c35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(inf, dtype=torch.float64, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctc_loss_bayes =  BayesianCTC(odim, eprojs, dropout_rate, reduce, log_semiring)\n",
        "B = 5\n",
        "T = 3\n",
        "D = 10\n",
        "\n",
        "# Create some random input tensors.\n",
        "hs_pad = torch.randn(B, T, D)\n",
        "hlens = torch.randint(0, T, (B,))\n",
        "ali = torch.randint(0, T, (B, T))"
      ],
      "metadata": {
        "id": "0S1f41Q6znkJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ctc_graphs = [\n",
        "#     torch.tensor([[0, 1, 10, 1036831949],[1, 2, 20, 1036831949],[2, 3, 30,1036831949],[2, 4, 40,1036831949],[3, 5, 50,1036831949],[4, 5, 60,1036831949],[5, 6, 70,1036831949],[6,7,-1,1036831949]], dtype=torch.int32)\n",
        "# ]\n",
        "ctc_graphs = torch.randint(1, odim, (B, 3))\n",
        "loss = ctc_loss_bayes(hs_pad, hlens, ctc_graphs,ali)"
      ],
      "metadata": {
        "id": "WQok4bx30L4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}